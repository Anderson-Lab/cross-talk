{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdkGRLgcFN1o"
   },
   "source": [
    "Example pipeline for text to KG (v1)\n",
    "\n",
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR=\"/mnt/clbp/paul/cross-talk/texts.examples/\"\n",
    "REPORT_DIR=\"/mnt/clbp/paul/cross-talk/texts.examples/reports\"\n",
    "#!mkdir {REPORT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wdK8WtAF8ES"
   },
   "source": [
    "## License Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "61uNqFkvF-UU"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"/mnt/clbp/jsl_lic.json\") as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1PE2aPkzz_Xg"
   },
   "outputs": [],
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install -q spark-nlp-display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "RpdYuemp0qwP",
    "outputId": "890f71a7-cca1-4806-f22f-8722ec15df0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 16:48:54 WARN Utils: Your hostname, paul resolves to a loopback address: 127.0.1.1; using 172.18.0.2 instead (on interface eth0)\n",
      "23/09/26 16:48:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/myuser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/myuser/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ab19bc20-accb-46e3-9985-9cb8397d612e;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/myuser/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 733ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 by [com.google.errorprone#error_prone_annotations;2.18.0] in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   77  |   0   |   0   |   5   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ab19bc20-accb-46e3-9985-9cb8397d612e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/13ms)\n",
      "23/09/26 16:48:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 5.0.2\n",
      "Spark NLP_JSL Version : 5.0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://paul:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc1bd8366e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "params = {\"spark.driver.memory\":\"26G\",\n",
    "          \"spark.kryoserializer.buffer.max\":\"2000M\",\n",
    "          \"spark.driver.maxResultSize\":\"2000M\"}\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coresolution of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanbert_base_coref download started this may take some time.\n",
      "Approximate size to download 540.1 MB\n",
      "[ | ]spanbert_base_coref download started this may take some time.\n",
      "Approximate size to download 540.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 16:49:31.078665: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules and classes\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "from sparknlp.annotator import (\n",
    "    SentenceDetector,\n",
    "    Tokenizer,\n",
    "    SpanBertCorefModel\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Step 1: Transforms raw texts to `document` annotation\n",
    "document = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\") \\\n",
    "            .setOutputCol(\"document\")\n",
    "\n",
    "# Step 2: Sentence Detection\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "            .setInputCols(\"document\") \\\n",
    "            .setOutputCol(\"sentences\")\n",
    "\n",
    "# Step 3: Tokenization\n",
    "token = Tokenizer() \\\n",
    "            .setInputCols(\"sentences\") \\\n",
    "            .setOutputCol(\"tokens\") \\\n",
    "            .setContextChars([\"(\", \")\", \"?\", \"!\", \".\", \",\", \":\"])\n",
    "\n",
    "# Step 4: Coreference Resolution\n",
    "corefResolution= SpanBertCorefModel().pretrained(\"spanbert_base_coref\")\\\n",
    "            .setInputCols([\"sentences\", \"tokens\"]) \\\n",
    "            .setOutputCol(\"corefs\") \\\n",
    "            .setCaseSensitive(False)\n",
    "            \n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[document, sentenceDetector, token, corefResolution])\n",
    "\n",
    "# Create the dataframe\n",
    "data = spark.createDataFrame([[\"Ana is a Graduate Student at UT Dallas. She loves working in Natural Language Processing at the Institute. Her hobbies include blogging, dancing and singing.\"]]).toDF(\"text\")\n",
    "\n",
    "# Fit the dataframe to the pipeline to get the model\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve(text,results):\n",
    "    pieces = []\n",
    "    from_pieces_for_eval = []\n",
    "    to_pieces_for_eval = []\n",
    "    current = 0\n",
    "    for ix in results.index:\n",
    "        row = results.loc[ix,\"coref\"]\n",
    "        begin,end = row['begin'],row['end']\n",
    "        metadata = row['metadata']\n",
    "        head = metadata['head']\n",
    "        if head != \"ROOT\":\n",
    "            text2_up_to = text[current:begin]\n",
    "            pieces.append(text2_up_to)\n",
    "            pieces.append(head)\n",
    "            \n",
    "            from_pieces_for_eval.append(text2_up_to)\n",
    "            from_pieces_for_eval.append(\"[\"+text[begin:end+1]+\"]\")\n",
    "\n",
    "            to_pieces_for_eval.append(text2_up_to)\n",
    "            to_pieces_for_eval.append(\"[\"+head+\"]\")\n",
    "\n",
    "            current = (end+1)\n",
    "    pieces.append(text[current:])\n",
    "    from_pieces_for_eval.append(text[current:])\n",
    "    to_pieces_for_eval.append(text[current:])\n",
    "    return \"\".join(pieces),\"\".join(from_pieces_for_eval),\"\".join(to_pieces_for_eval)\n",
    "\n",
    "def resolveText2Text(text):\n",
    "    data_2 = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "    model = pipeline.fit(data_2)\n",
    "\n",
    "    #results = model.transform(data_2).selectExpr(\"explode(corefs) AS coref\").toPandas()\n",
    "    #text2,from_text,to_text = resolve(text,results)\n",
    "    try:\n",
    "        results = model.transform(data_2).selectExpr(\"explode(corefs) AS coref\").toPandas()\n",
    "        text2,from_text,to_text = resolve(text,results)\n",
    "    except:\n",
    "        print(\"Failed on the following paragraph:\")\n",
    "        print(text)        \n",
    "        text2 = text\n",
    "        from_text,to_text = text,text\n",
    "    return text2,from_text,to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/mnt/clbp/paul/cross-talk/texts.examples//coref_resolved/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir $SRC_DIR/coref_resolved/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/clbp/paul/cross-talk/texts.examples/physical_activity.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/clbp/paul/cross-talk/texts.examples/spinal_cord.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/clbp/paul/cross-talk/texts.examples/personal_medical_history.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "path = SRC_DIR\n",
    "files = glob.glob(path + '/*.json')\n",
    "changed_sentences = []\n",
    "unchanged_sentences = []\n",
    "for file in files:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    paragraphs = data['text'].split(\"\\n\\n\")\n",
    "    new_paragraphs = []\n",
    "    for text in paragraphs:\n",
    "        text2,from_text,to_text = resolveText2Text(text)\n",
    "        if text2 != text:\n",
    "            changed_sentences.append([text,from_text,to_text,\"?\"])\n",
    "            #print(\"Orig text:\",text)\n",
    "            #print(\"From text:\",from_text)\n",
    "            #print(\"To text  :\",to_text)\n",
    "            #print()\n",
    "        else:\n",
    "            unchanged_sentences.append(text)\n",
    "        new_paragraphs.append(text2)\n",
    "    new_file = path+\"/coref_resolved/\"+file.split(\"/\")[-1]\n",
    "    with open(new_file,\"w\") as f:\n",
    "        data['text'] = \"\\n\\n\".join(new_paragraphs)\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "    orig_contents = json.loads(open(file).read())\n",
    "    orig_sentences = orig_contents[\"text\"].split(\".\")\n",
    "    new_contents = json.loads(open(new_file).read())\n",
    "    new_sentences = new_contents[\"text\"].split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanbert_base_coref download started this may take some time.\n",
      "Approximate size to download 540.1 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 1: Transforms raw texts to `document` annotation\n",
    "document = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\") \\\n",
    "            .setOutputCol(\"document\")\n",
    "\n",
    "# Step 2: Sentence Detection\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "            .setInputCols(\"document\") \\\n",
    "            .setOutputCol(\"sentences\")\n",
    "\n",
    "# Step 3: Tokenization\n",
    "token = Tokenizer() \\\n",
    "            .setInputCols(\"sentences\") \\\n",
    "            .setOutputCol(\"tokens\") \\\n",
    "            .setContextChars([\"(\", \")\", \"?\", \"!\", \".\", \",\"])\n",
    "\n",
    "\n",
    "# Step 4: Coreference Resolution\n",
    "corefResolution= SpanBertCorefModel().pretrained(\"spanbert_base_coref\")\\\n",
    "            .setInputCols([\"sentences\", \"tokens\"]) \\\n",
    "            .setOutputCol(\"corefs\") \\\n",
    "            .setCaseSensitive(False)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[document, sentenceDetector, token, corefResolution])\n",
    "\n",
    "# Create the dataframe\n",
    "data = spark.createDataFrame([[\"The spinal column is biomechanically stabilized by three subsystems: 1) passive subsystem that includes bone, cartilage, ligaments, intervertebral disc; 2) active subsystem that includes the paraspinal muscles; and 3) the neural control subsystem. These subsystems are often conceptualized separately, but they are functionally interdependent. Motor control and function include muscle recruitment, strength, and endurance. An important feedback component of neuromotor control is proprioception, which refers to afferent information arising from internal peripheral areas that contribute to postural control, joint stability, and several conscious sensations. Movement and control disorders presumably lead to a proprioceptive deficit, because of stress on local muscle spindles and joint receptors in the painful area resulting from stress to a joint caused by an individual’s maladaptive movement. Subsequently, abnormal joint and tissue loading during daily activities and postures may affect local proprioceptors and maintain this vicious cycle. Changes in muscle activity have been linked to spinal pain (muscle-tension or pain-spasm-pain model) or restriction of spinal motion (pain adaptation).\"]]).toDF(\"text\")\n",
    "\n",
    "# Fit the dataframe to the pipeline to get the model\n",
    "model = pipeline.fit(data)\n",
    "\n",
    "results = model.transform(data).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = open(\"/mnt/clbp/.openai_api_key.txt\").read().strip()\n",
    "\n",
    "max_tokens = 4097\n",
    "min_num_words_in_sentences = 4\n",
    "min_num_words_in_paragraph = min_num_words_in_sentences\n",
    "min_len_sentence = min_num_words_in_sentences*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/mnt/clbp/paul/cross-talk/texts.examples//coref_resolved/triplets/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir $SRC_DIR/coref_resolved/triplets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "import glob\n",
    "\n",
    "min_num_words_in_paragraph = 3\n",
    "\n",
    "def get_triplets(contents,field):\n",
    "    all_sentences_with_results = []\n",
    "    all_sentences_without_results = []\n",
    "    \n",
    "    output = []\n",
    "    paragraphs = contents[field].split(\"\\n\\n\")\n",
    "    for paragraph in paragraphs:\n",
    "        output.append({\"paragraph\": {\"text\":paragraph}})\n",
    "        num_words = len(paragraph.split(\" \"))\n",
    "        if num_words < min_num_words_in_paragraph:\n",
    "            continue\n",
    "\n",
    "        sentences_with_results = []\n",
    "        sentences_without_results = []\n",
    "        for sent in paragraph.split(\".\"):\n",
    "            print(sent[:10])\n",
    "            if sent.strip() == \"\":\n",
    "                continue\n",
    "            messages=[\n",
    "                  {\"role\": \"system\", \"content\": \"Rewrite a sentence such that each sentence has one subject and one object. Keep the meaning the same and write in active voice. Your answer should be a numbered list.\"},\n",
    "                  {\"role\": \"user\", \"content\": sent }\n",
    "            ]\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                print(\"Trying openai 1\")\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=messages,\n",
    "                      temperature=0,\n",
    "                      max_tokens=1024,\n",
    "                      top_p=1,\n",
    "                      frequency_penalty=0,\n",
    "                      presence_penalty=0\n",
    "                    )\n",
    "                    finished = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            sentences = response['choices'][0]['message']['content'].split(\"\\n\")\n",
    "            if sentences[0].strip() == \"None\":\n",
    "                continue\n",
    "            sentences_with_results += [\" \".join(s.split(\" \")[1:]) for s in sentences] # Remove the number\n",
    "            \n",
    "        output[-1][\"paragraph\"][\"sentences_with_results\"] = [{\"text\":s, \"clauses\": []} for s in sentences_with_results]\n",
    "        output[-1][\"paragraph\"][\"sentences_without_results\"] = [{\"text\":s, \"clauses\": []} for s in sentences_without_results]\n",
    "\n",
    "        print(\"Sentences with results:\")\n",
    "        print(\"\\n\".join(sentences_with_results))\n",
    "        for j,sentence in enumerate(sentences_with_results):\n",
    "            print(\"Sentence\",j+1,\"out of\",len(sentences_with_results))\n",
    "\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                print(\"Trying openai 2\")\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"Break down this sentence into more straightforward sentences. Your answer should be a numbered list.\"},\n",
    "                            {\"role\": \"user\", \"content\": sentence }\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                        max_tokens=1024,\n",
    "                        top_p=1,\n",
    "                        frequency_penalty=0,\n",
    "                        presence_penalty=0\n",
    "                    )\n",
    "                    finished = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            time.sleep(1)\n",
    "            clauses = [\" \".join(clause.split(\" \")[1:]) for clause in response['choices'][0]['message']['content'].split(\"\\n\")]\n",
    "            paragraph_clauses = \" \".join(clauses)\n",
    "            clauses = []\n",
    "            text2,from_text,to_text = resolveText2Text(paragraph_clauses)\n",
    "            for c in text2.strip().split(\".\"):\n",
    "                c = c.strip()\n",
    "                if c == \"\":\n",
    "                    continue\n",
    "                clauses.append(f\"{c}.\")\n",
    "            output[-1][\"paragraph\"][\"sentences_with_results\"][j][\"clauses\"] = [{\"text\": c} for c in clauses]\n",
    "            for i, clause in enumerate(clauses):  \n",
    "                print(\"Clause\",i+1,\"out of\",len(clauses))\n",
    "                finished = False\n",
    "                while not finished:\n",
    "                    print(\"Trying openai 3\")\n",
    "                    try:\n",
    "                        response = openai.ChatCompletion.create(\n",
    "                            model=\"gpt-3.5-turbo\",\n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": \"You will be provided a sentence, and your task is split it into a subject, verb, and object. Return your answer as JSON with keys subject, verb, object.\"},\n",
    "                                {\"role\": \"user\", \"content\": clause }\n",
    "                            ],\n",
    "                            temperature=0,\n",
    "                            max_tokens=1024,\n",
    "                            top_p=1,\n",
    "                            frequency_penalty=0,\n",
    "                            presence_penalty=0\n",
    "                        )\n",
    "                        finished = True\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                time.sleep(1)\n",
    "                content = response['choices'][0]['message']['content']\n",
    "                output[-1][\"paragraph\"][\"sentences_with_results\"][j][\"clauses\"][i][\"triplet\"] = json.loads(content)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying openai 1\n",
      "Trying openai 1\n",
      "Sentences with results:\n",
      "The patient's personal history includes personal preferences.\n",
      "The patient's personal history includes expectations.\n",
      "The patient's personal history includes habits, such as sleep schedule.\n",
      "The patient's personal history includes habits, such as clothing choices.\n",
      "The patient's personal history includes habits, such as diet.\n",
      "Games are included in a patient's personal history.\n",
      "Sports are included in a patient's personal history.\n",
      "Relationships are included in a patient's personal history.\n",
      "Sentence 1 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 2 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 3 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 4 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 5 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 6 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 7 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 8 out of 8\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 4\n",
      "Trying openai 3\n",
      "Clause 2 out of 4\n",
      "Trying openai 3\n",
      "Clause 3 out of 4\n",
      "Trying openai 3\n",
      "Clause 4 out of 4\n",
      "Trying openai 3\n"
     ]
    }
   ],
   "source": [
    "contents = {\"text\": \"A patient's personal history also includes personal preferences, expectations, habits, such as sleep schedule, clothing choices, and diet. A patient's personal history also includes games, sports, and relationships.\"}\n",
    "outputs = get_triplets(contents,\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': {'text': \"A patient's personal history also includes personal preferences, expectations, habits, such as sleep schedule, clothing choices, and diet. A patient's personal history also includes games, sports, and relationships.\",\n",
       "   'sentences_with_results': [{'text': \"The patient's personal history includes personal preferences.\",\n",
       "     'clauses': [{'text': 'The patient has a personal history.',\n",
       "       'triplet': {'subject': 'patient',\n",
       "        'verb': 'has',\n",
       "        'object': 'a personal history'}},\n",
       "      {'text': 'a personal history includes personal preferences.',\n",
       "       'triplet': {'subject': 'a personal history',\n",
       "        'verb': 'includes',\n",
       "        'object': 'personal preferences'}}]},\n",
       "    {'text': \"The patient's personal history includes expectations.\",\n",
       "     'clauses': [{'text': 'The patient has a personal history.',\n",
       "       'triplet': {'subject': 'patient',\n",
       "        'verb': 'has',\n",
       "        'object': 'a personal history'}},\n",
       "      {'text': 'a personal history includes expectations.',\n",
       "       'triplet': {'subject': 'a personal history',\n",
       "        'verb': 'includes',\n",
       "        'object': 'expectations'}}]},\n",
       "    {'text': \"The patient's personal history includes habits, such as sleep schedule.\",\n",
       "     'clauses': [{'text': 'The patient has a personal history.',\n",
       "       'triplet': {'subject': 'patient',\n",
       "        'verb': 'has',\n",
       "        'object': 'a personal history'}},\n",
       "      {'text': 'a personal history includes habits.',\n",
       "       'triplet': {'subject': 'a personal history',\n",
       "        'verb': 'includes',\n",
       "        'object': 'habits'}},\n",
       "      {'text': 'One of the habits included in a personal history is the sleep schedule.',\n",
       "       'triplet': {'subject': 'One of the habits',\n",
       "        'verb': 'is included',\n",
       "        'object': 'the sleep schedule'}}]},\n",
       "    {'text': \"The patient's personal history includes habits, such as clothing choices.\",\n",
       "     'clauses': [{'text': 'The patient has a personal history.',\n",
       "       'triplet': {'subject': 'patient',\n",
       "        'verb': 'has',\n",
       "        'object': 'a personal history'}},\n",
       "      {'text': 'a personal history includes habits.',\n",
       "       'triplet': {'subject': 'a personal history',\n",
       "        'verb': 'includes',\n",
       "        'object': 'habits'}},\n",
       "      {'text': 'One of the habits included in a personal history is clothing choices.',\n",
       "       'triplet': {'subject': 'One of the habits',\n",
       "        'verb': 'is',\n",
       "        'object': 'clothing choices'}}]},\n",
       "    {'text': \"The patient's personal history includes habits, such as diet.\",\n",
       "     'clauses': [{'text': 'The patient has a personal history.',\n",
       "       'triplet': {'subject': 'patient',\n",
       "        'verb': 'has',\n",
       "        'object': 'a personal history'}},\n",
       "      {'text': 'a personal history includes habits.',\n",
       "       'triplet': {'subject': 'a personal history',\n",
       "        'verb': 'includes',\n",
       "        'object': 'habits'}},\n",
       "      {'text': 'One of the habits included in a personal history is the patient diet.',\n",
       "       'triplet': {'subject': 'One of the habits',\n",
       "        'verb': 'is included',\n",
       "        'object': 'the patient diet'}}]},\n",
       "    {'text': \"Games are included in a patient's personal history.\",\n",
       "     'clauses': [{'text': \"Games are part of a patient's personal history.\",\n",
       "       'triplet': {'subject': 'Games',\n",
       "        'verb': 'are',\n",
       "        'object': \"part of a patient's personal history\"}},\n",
       "      {'text': 'Games are included or mentioned in a patient personal history.',\n",
       "       'triplet': {'subject': 'Games',\n",
       "        'verb': 'are included or mentioned in',\n",
       "        'object': 'a patient personal history'}},\n",
       "      {'text': 'a patient personal history includes games.',\n",
       "       'triplet': {'subject': 'patient personal history',\n",
       "        'verb': 'includes',\n",
       "        'object': 'games'}}]},\n",
       "    {'text': \"Sports are included in a patient's personal history.\",\n",
       "     'clauses': [{'text': \"A patient's personal history includes sports.\",\n",
       "       'triplet': {'subject': \"patient's personal history\",\n",
       "        'verb': 'includes',\n",
       "        'object': 'sports'}},\n",
       "      {'text': \"Sports are part of a patient's personal history.\",\n",
       "       'triplet': {'subject': 'Sports',\n",
       "        'verb': 'are',\n",
       "        'object': \"part of a patient's personal history\"}}]},\n",
       "    {'text': \"Relationships are included in a patient's personal history.\",\n",
       "     'clauses': [{'text': \"Relationships are a part of a patient's personal history.\",\n",
       "       'triplet': {'subject': 'Relationships',\n",
       "        'verb': 'are',\n",
       "        'object': \"a part of a patient's personal history\"}},\n",
       "      {'text': \"Relationships are included when documenting a patient's personal history.\",\n",
       "       'triplet': {'subject': 'Relationships',\n",
       "        'verb': 'are included',\n",
       "        'object': \"when documenting a patient's personal history\"}},\n",
       "      {'text': \"When discussing a patient's personal history, relationships are taken into account.\",\n",
       "       'triplet': {'subject': 'relationships',\n",
       "        'verb': 'are taken into account',\n",
       "        'object': \"when discussing a patient's personal history\"}},\n",
       "      {'text': 'The personal history of a patient includes information about a patient relationships.',\n",
       "       'triplet': {'subject': 'The personal history of a patient',\n",
       "        'verb': 'includes',\n",
       "        'object': \"information about a patient's relationships\"}}]}],\n",
       "   'sentences_without_results': []}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 myuser myuser 9960 Sep 26 21:29 /mnt/clbp/paul/cross-talk/texts.examples//coref_resolved/triplets/personal_medical_history.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l {outfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/clbp/paul/cross-talk/texts.examples//coref_resolved/spinal_cord.json\n",
      "The spinal\n",
      "Trying openai 1\n",
      " three sub\n",
      "Trying openai 1\n",
      " Motor con\n",
      "Trying openai 1\n",
      " An import\n",
      "Trying openai 1\n",
      " Movement \n",
      "Trying openai 1\n",
      " Subsequen\n",
      "Trying openai 1\n",
      " Changes i\n",
      "Trying openai 1\n",
      "\n",
      "Sentences with results:\n",
      "Three subsystems biomechanically stabilize the spinal column: the passive subsystem includes bone, cartilage, ligaments, and intervertebral disc.\n",
      "The paraspinal muscles form the active subsystem that biomechanically stabilizes the spinal column.\n",
      "The neural control subsystem also plays a role in biomechanically stabilizing the spinal column.\n",
      "People often conceptualize three subsystems separately, but the paraspinal muscles are functionally interdependent.\n",
      "Many individuals tend to conceptualize three subsystems separately, but the paraspinal muscles are functionally interdependent.\n",
      "It is common for individuals to conceptualize three subsystems separately, but the paraspinal muscles are functionally interdependent.\n",
      "Muscle recruitment is included in motor control and function.\n",
      "Strength is included in motor control and function.\n",
      "Endurance is included in motor control and function.\n",
      "Proprioception is an important feedback component of neuromotor control.\n",
      "Proprioception refers to afferent information arising from internal peripheral areas.\n",
      "Proprioception contributes to postural control, joint stability, and several conscious sensations.\n",
      "Presumably, movement and control disorders lead to a proprioceptive deficit.\n",
      "Stress on local muscle spindles and joint receptors in the painful area results from stress to a joint caused by an individual's maladaptive movement.\n",
      "Abnormal joint and tissue loading during daily activities and postures may affect local proprioceptors.\n",
      "This may maintain the vicious cycle.\n",
      "Researchers have linked changes in muscle activity to spinal pain, specifically the muscle-tension or pain-spasm-pain model.\n",
      "The pain adaptation theory suggests that changes in muscle activity can be linked to the restriction of spinal motion and subsequent spinal pain.\n",
      "Sentence 1 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 2 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 5\n",
      "Trying openai 3\n",
      "Clause 2 out of 5\n",
      "Trying openai 3\n",
      "Clause 3 out of 5\n",
      "Trying openai 3\n",
      "Clause 4 out of 5\n",
      "Trying openai 3\n",
      "Clause 5 out of 5\n",
      "Trying openai 3\n",
      "Sentence 3 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 4 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 5 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 6 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 7 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 8 out of 18\n",
      "Trying openai 2\n",
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 9 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 10 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 11 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 12 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 13 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 8\n",
      "Trying openai 3\n",
      "Clause 2 out of 8\n",
      "Trying openai 3\n",
      "Clause 3 out of 8\n",
      "Trying openai 3\n",
      "Clause 4 out of 8\n",
      "Trying openai 3\n",
      "Clause 5 out of 8\n",
      "Trying openai 3\n",
      "Clause 6 out of 8\n",
      "Trying openai 3\n",
      "Clause 7 out of 8\n",
      "Trying openai 3\n",
      "Clause 8 out of 8\n",
      "Trying openai 3\n",
      "Sentence 14 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 4\n",
      "Trying openai 3\n",
      "Clause 2 out of 4\n",
      "Trying openai 3\n",
      "Clause 3 out of 4\n",
      "Trying openai 3\n",
      "Clause 4 out of 4\n",
      "Trying openai 3\n",
      "The server is overloaded or not ready yet.\n",
      "Trying openai 3\n",
      "Sentence 15 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 16 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 17 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 18 out of 18\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "/mnt/clbp/paul/cross-talk/texts.examples//coref_resolved/personal_medical_history.json\n",
      "The person\n",
      "Trying openai 1\n",
      " the perso\n",
      "Trying openai 1\n",
      " The patie\n",
      "Trying openai 1\n",
      " All of th\n",
      "Trying openai 1\n",
      "\n",
      "Sentences with results:\n",
      "A patient's personal history includes their employment status.\n",
      "A patient's personal history includes their occupation.\n",
      "A patient's personal history includes their marital status.\n",
      "A patient's personal history includes their insurance status.\n",
      "A patient's personal history includes social history, which encompasses alcohol, tobacco, and substance use.\n",
      "A patient's personal history also includes personal preferences, expectations, and habits such as sleep schedule, clothing choices, and diet.\n",
      "The medical history of the patient's isolated and chronic illnesses is important.\n",
      "The isolated and chronic illnesses of the patient have an important medical history.\n",
      "Each individual likely experiences pain based on the contributions of all these entities.\n",
      "All these entities likely play a role in how each individual experiences pain.\n",
      "The experience of pain in each individual is likely influenced by all these entities.\n",
      "How pain is experienced in each individual is likely influenced by all these entities.\n",
      "All these entities likely have an impact on the experience of pain in each individual.\n",
      "Sentence 1 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 4\n",
      "Trying openai 3\n",
      "Clause 2 out of 4\n",
      "Trying openai 3\n",
      "Clause 3 out of 4\n",
      "Trying openai 3\n",
      "Clause 4 out of 4\n",
      "Trying openai 3\n",
      "Sentence 2 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 5\n",
      "Trying openai 3\n",
      "Clause 2 out of 5\n",
      "Trying openai 3\n",
      "Clause 3 out of 5\n",
      "Trying openai 3\n",
      "Clause 4 out of 5\n",
      "Trying openai 3\n",
      "Clause 5 out of 5\n",
      "Trying openai 3\n",
      "Sentence 3 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 4 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 5 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 4\n",
      "Trying openai 3\n",
      "Clause 2 out of 4\n",
      "Trying openai 3\n",
      "Clause 3 out of 4\n",
      "Trying openai 3\n",
      "Clause 4 out of 4\n",
      "Trying openai 3\n",
      "Sentence 6 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 7 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 8 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 9 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n",
      "Sentence 10 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 4\n",
      "Trying openai 3\n",
      "Clause 2 out of 4\n",
      "Trying openai 3\n",
      "Clause 3 out of 4\n",
      "Trying openai 3\n",
      "Clause 4 out of 4\n",
      "Trying openai 3\n",
      "Sentence 11 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 12 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 3\n",
      "Trying openai 3\n",
      "Clause 2 out of 3\n",
      "Trying openai 3\n",
      "Clause 3 out of 3\n",
      "Trying openai 3\n",
      "Sentence 13 out of 13\n",
      "Trying openai 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1 out of 2\n",
      "Trying openai 3\n",
      "Clause 2 out of 2\n",
      "Trying openai 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/27 04:02:50 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 176025 ms exceeds timeout 120000 ms\n",
      "23/09/27 04:02:50 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "path = SRC_DIR\n",
    "files = glob.glob(path + '/coref_resolved/*.json')\n",
    "for file in files[1:]:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    output = get_triplets(data,\"text\")\n",
    "    outfile = path + '/coref_resolved/triplets/'+file.split(\"/\")[-1]\n",
    "    open(outfile,\"w\").write(json.dumps(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplets to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/clbp/paul/cross-talk/texts.examples/coref_resolved/triplets/physical_activity.json\n",
      "physical activity|imposes|loads and movements onto the spine\n",
      "loads and movements|are|not constant, but instead fluctuate\n",
      "fluctuating loads and movements|can have|an impact on the spine\n",
      "physical activity|involves|putting stress and strain on the spine\n",
      "stress and strain|cause|the spine to move in different ways\n",
      "movements of the spine|are|not constant, but rather change or vary\n",
      "the spine|is|a part of the body that is subjected to various loads and movements\n",
      "various loads and movements|are caused by|physical activity\n",
      "the spine|experiences|various loads and movements\n",
      "loading in literature|has shown|that it has an impact that depends on the dose\n",
      "sedentary and strenuous activities|are believed to have|negative effects\n",
      "literature|has shown|that loading has a dose-dependent influence\n",
      "sedentary activities|are thought|to be detrimental\n",
      "strenuous activities|are|also thought to be detrimental\n",
      "the influence of loading in literature|is|dose-dependent\n",
      "sedentary activities|are thought|to be detrimental\n",
      "strenuous activities|are|also thought to be detrimental\n",
      "literature|suggests|that loading has a dose-dependent influence\n",
      "sedentary activities|are thought|to be detrimental\n",
      "strenuous activities|are|also thought to be detrimental\n",
      "occupational activities|refer to|the tasks and responsibilities that people perform in people jobs\n",
      "tasks and responsibilities|perform|various categories of physical activity\n",
      "physical activity categories|can include|tasks that require physical exertion or movement\n",
      "examples of physical activity categories in occupational activities|may include|lifting heavy objects, walking or standing for long periods, or operating machinery\n",
      "recreational activities|are|activities that people do for fun and enjoyment\n",
      "recreational activities|can include|different categories of physical activity\n",
      "sports-related activities|refer to|activities that are related to sports\n",
      "activities|may include|different categories of physical activity\n",
      "the world health organization|is|an international organization\n",
      "the world health organization|has|a definition for physical activity\n",
      "physical activity|refers to|any bodily movement\n",
      "any bodily movement|is produced by|skeletal muscle\n",
      "skeletal muscle|is responsible for|producing bodily movement\n",
      "bodily movement|is|a result of the actions of skeletal muscle\n",
      "bodily movement|refers to|any physical activity or exercise that involves the movement of the body\n",
      "resting energy expenditure|is|the amount of energy or calories that the body burns while at rest\n",
      "there|is|a significant increase in the amount of energy that the body burns compared to when the body is at rest\n",
      "the concept of physical activity|is|about what people are able to do or actually do in people daily lives\n",
      "the concept of physical activity|emphasizes|the actions and abilities of individuals in terms of physical movement\n",
      "the concept of physical activity|encompasses|both the potential and actual engagement in physical activities\n",
      "the concept of physical activity|is concerned with|the activities that people perform as part of people everyday routines\n",
      "disability|is|a concept that centers around the limitations or restrictions of individuals\n",
      "disability|emphasizes|the things that people are unable to do or struggle with due to people condition or impairment\n",
      "three studies|were conducted|\n",
      "three studies|reported|data\n",
      "data|was related to|physical functioning\n",
      "data|was|also related to work status\n",
      "data|was|further related to disability\n",
      "data|was related|to pain severity\n",
      "two studies|were conducted|\n",
      "two studies|had|little confidence in the strength of evidence (soe)\n",
      "two studies|reported|data\n",
      "data|was related to|physical functioning\n",
      "physical functioning|was assessed|using a questionnaire or accelerometry\n",
      "dataphysical functioning|was assessed|using a questionnaire or accelerometry\n",
      "the data|was|also related to work status, disability, and pain severity\n",
      "there|were|two studies that reported data\n",
      "two studies|reported|data\n",
      "the data reported|was related to|activity in relation to work status, disability, and pain severity\n",
      "there|were|two studies that reported data\n",
      "two studies|reported|data\n",
      "data|was related to|activity, as assessed by questionnaire or accelerometry\n",
      "data|looked at|work status, disability, and pain severity\n",
      "researchers|are discussing|potential mechanisms\n",
      "potential mechanisms|include|the fear-avoidance model\n",
      "another potential mechanism|being discussed|the avoidance-endurance model\n",
      "the fear-avoidance model|suggests|that patients avoid activities\n",
      "the avoidance-endurance model|suggests|that patients avoid activities\n",
      "both models|hypothesize|that fear or avoidance plays a role in patients' activity avoidance\n",
      "patients|have|a fear of reinjury\n",
      "fear|causes|patients to avoid certain activities\n",
      "fear of reinjury|is|a feeling of being afraid that a previous injury might happen again\n",
      "deconditioning|refers to|a decrease in physical fitness or strength\n",
      "fear of reinjury|can cause|deconditioning\n",
      "deconditioning|is|a factor that contributes to disability\n",
      "deconditioning|can make|a person's disability worse\n",
      "someone|becomes|deconditioned\n",
      "disability|is worsened by|the process of deconditioning\n",
      "/mnt/clbp/paul/cross-talk/texts.examples/coref_resolved/triplets/spinal_cord.json\n",
      "the spinal column|is biomechanically stabilized|by three subsystems\n",
      "the passive subsystem|is|one of three subsystems\n",
      "passive subsystem|includes|bone, cartilage, ligaments, and intervertebral discs\n",
      "the paraspinal muscles|are|a group of muscles\n",
      "the paraspinal muscles|are responsible for|stabilizing the spinal column\n",
      "paraspinal muscles|form|active subsystem\n",
      "biomechanically|refers to|the way in which the paraspinal muscles stabilizing\n",
      "biomechanically|refers|to the way in which the muscles function in relation to the body's mechanics\n",
      "the neural control subsystem|is involved in|stabilizing the spinal column\n",
      "the neural control subsystem|has|a role in biomechanically stabilizing the spinal column\n",
      "people|think|of three subsystems separately\n",
      "the paraspinal muscles|are|functionally interdependent\n",
      "many individuals|tend to conceptualize|three subsystems separately\n",
      "the paraspinal muscles|are|functionally interdependent\n",
      "many people|tend to think|of three subsystems separately\n",
      "paraspinal muscles|are|functionally interdependent\n",
      "muscle recruitment|is|a component of motor control and function\n",
      "motor control and function|encompass|muscle recruitment\n",
      "motor control and function|involve|strength\n",
      "strength|is|a component of motor control and function\n",
      "motor control and function|include|endurance\n",
      "endurance|is|a component of motor control and function\n",
      "proprioception|is|a feedback component\n",
      "proprioception|is|important for neuromotor control\n",
      "proprioception|is|a term that describes a type of sensory information\n",
      "this sensory information|is called|afferent information\n",
      "afferent information|refers to|signals that travel from the body's internal peripheral areas to the brain\n",
      "proprioception|is|a factor that contributes to postural control\n",
      "proprioception|is involved|in joint stability\n",
      "proprioception|plays|a role in several conscious sensations\n",
      "movement and control disorders|are|conditions that affect a person's ability to move and control a person's body\n",
      "movement and control disorders|can result in|a deficit in proprioception\n",
      "proprioception|refers to|the body's ability to sense its position and movement in space\n",
      "there|is|a deficit in proprioception\n",
      "this deficit|is believed|to be caused by movement and control disorders, the body's ability to sense its position and movement in space\n",
      "deficit|means|that the person has difficulty perceiving and understanding where their body is in relation to its surroundings\n",
      "a deficit in proprioception|is|the person body in relation to their body surroundings\n",
      "this deficit|is believed|to be caused by movement and control disorders\n",
      "stress|is placed on|local muscle spindles and joint receptors\n",
      "stress|occurs|in the painful area\n",
      "stress|is caused by|an individual's maladaptive movement\n",
      "an individual maladaptive movement|leads to|stress on a joint\n",
      "abnormal joint and tissue loading|can occur|during daily activities and postures\n",
      "abnormal joint and tissue loading|may have|an impact on local proprioceptors\n",
      "maintaining the vicious cycle|is|a possibility\n",
      "maintaining|contribute|the vicious cycle\n",
      "changes in muscle activity|have been linked to|spinal pain\n",
      "the specific model|explains|linked\n",
      "muscle tension or spasms|can cause|pain in the spine\n",
      "the pain adaptation theory|suggests|that changes in muscle activity are connected to the restriction of spinal motion\n",
      "restriction of spinal motion|can lead to|spinal pain\n",
      "the pain adaptation theory|proposes|that changes in muscle activity are a contributing factor to spinal pain\n",
      "/mnt/clbp/paul/cross-talk/texts.examples/coref_resolved/triplets/personal_medical_history.json\n",
      "a patient's personal history|is|a record of their past experiences and background information\n",
      "one aspect|is|their employment status\n",
      "employment status|refers to|whether the patient is currently employed, unemployed, or retired\n",
      "employment status|is|important for understanding the patient's financial situation and potential sources of stress or support\n",
      "a patient's personal history|is|a collection of information about their life\n",
      "one aspect|is|occupation\n",
      "the occupation|refers to|the job or profession that the patient is engaged in\n",
      "occupation|refers to|the job or profession that the patient is engaged in\n",
      "the patient's occupation|is|an important detail to consider when assessing their overall health and well-being\n",
      "a patient's personal history|is|a collection of information about them\n",
      "one aspect of a patient's personal history|is|their marital status\n",
      "a patient's personal history|is|a collection of information about them\n",
      "one aspect of a patient's personal history|is|their insurance status\n",
      "patient's personal history|includes|social history\n",
      "social history|encompasses|alcohol use\n",
      "social history|encompasses|tobacco use\n",
      "social history|encompasses|substance use\n",
      "patient's personal history|includes|personal preferences, expectations, and habits\n",
      "personal preferences, expectations, and habits|can include|things like sleep schedule, clothing choices, and diet\n",
      "the patient|has|isolated and chronic illnesses\n",
      "the medical history|is|important\n",
      "patient|has|isolated illnesses\n",
      "the patient|has|chronic illnesses\n",
      "the patient's medical history|is|important\n",
      "pain|is experienced|by each individual\n",
      "the experience of pain|is influenced by|the contributions of all these entities\n",
      "entities|are involved|how each individual experiences pain\n",
      "multiple entities|influence|the way pain is perceived by an individual\n",
      "the role|played|by multiple entities that are likely involved in how each individual experiences pain\n",
      "involvement of multiple entities|is likely to contribute|unique experience of pain for each individual\n",
      "the experience of pain|is influenced|by various factors\n",
      "various factors|can vary|from person to person\n",
      "each individual's experience of pain|is likely influenced|by various factors\n",
      "pain|is experienced|differently by each individual\n",
      "the experience of pain|is|likely influenced by various factors\n",
      "various factors|can include|different entities\n",
      "entities|affect|experience of pain\n",
      "multiple entities|affect|the experience of pain in each person\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from py2neo import Graph\n",
    "graph = Graph(\"bolt://localhost:7687\",password='12345678',name=\"version1\")\n",
    "\n",
    "tx = graph.begin()\n",
    "#tx.run(\"MATCH (n:Paragraph) DETACH DELETE n;\")\n",
    "#tx.run(\"MATCH (n:ResultSentence) DETACH DELETE n;\")\n",
    "#tx.run(\"MATCH (n:Subject) DETACH DELETE n;\")\n",
    "#tx.run(\"MATCH (n:Object) DETACH DELETE n;\")\n",
    "#tx.run(\"MATCH (n:Entity) DETACH DELETE n;\")\n",
    "tx.run(\"MATCH (n) DETACH DELETE n;\")\n",
    "graph.commit(tx)\n",
    "\n",
    "path = '/mnt/clbp/paul/cross-talk/texts.examples/coref_resolved/triplets/'\n",
    "files = glob.glob(path+'/*.json')\n",
    "for file in files:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    name = file.split(\"/\")[-1]\n",
    "    domain = name.split(\".\")[0]\n",
    "    tx = graph.begin()\n",
    "    for ix,item in enumerate(data):\n",
    "        paragraph = item['paragraph']\n",
    "        text = paragraph['text']\n",
    "        tx.run(\"CREATE (paragraph:Paragraph {source: $source, domain: $domain, ix: $ix, text: $text}) RETURN paragraph\", source=file.split(\"/\")[-1], domain=domain, ix=ix, text=text)\n",
    "    graph.commit(tx)\n",
    "    tx = graph.begin()\n",
    "    for ix,item in enumerate(data):\n",
    "        paragraph = item['paragraph']\n",
    "        if 'sentences_with_results' in paragraph:\n",
    "            for jx, result_sentence in enumerate(paragraph['sentences_with_results']):\n",
    "                text = result_sentence['text']\n",
    "                tx.run(\"MATCH (paragraph:Paragraph {ix: $ix, domain:$domain}) CREATE (sentence:ResultSentence {ix: $jx, text: $text})-[:FROM]->(paragraph) RETURN *\", domain=domain, ix=ix, jx=jx, text=text)\n",
    "    graph.commit(tx)\n",
    "    tx = graph.begin()\n",
    "    for ix,item in enumerate(data):\n",
    "        paragraph = item['paragraph']\n",
    "        if 'sentences_with_results' in paragraph:\n",
    "            for jx, result_sentence in enumerate(paragraph['sentences_with_results']):\n",
    "                for kx, clause in enumerate(result_sentence['clauses']):\n",
    "                    text = clause['text']\n",
    "                    triplet = clause['triplet']\n",
    "                    subject,verb,object = triplet['subject'].lower(),triplet['verb'].lower(),triplet['object'].lower()\n",
    "                    print(subject,verb,object,sep=\"|\")\n",
    "                    tx.run(\"\"\"MATCH (sentence:ResultSentence {ix:$jx})-[:FROM]->(paragraph:Paragraph {ix: $ix, domain:$domain}) \n",
    "                              CREATE (s:Subject {ix: $kx, text: $subject, from: elementId(sentence)})-[v:VERB {ix: $kx, text: $verb, from: elementId(sentence)}]->(o:Object {ix: $kx, text: $object, from: elementId(sentence)}) RETURN *\"\"\",\n",
    "                           domain=domain, ix=ix, jx=jx, kx=kx, subject=subject, verb=verb, object=object)\n",
    "    graph.commit(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Clinical embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining/registering nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_biobert_clinical_base_cased download started this may take some time.\n",
      "Approximate size to download 386.6 MB\n",
      "[ | ]sent_biobert_clinical_base_cased download started this may take some time.\n",
      "Approximate size to download 386.6 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import SentenceDetector, BertSentenceEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "embeddings = BertSentenceEmbeddings.pretrained(\"sent_biobert_clinical_base_cased\", \"en\").setInputCols(\"sentence\").setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler,\n",
    "                            sentence,\n",
    "                            embeddings])\n",
    "\n",
    "def get_embedding(text):\n",
    "    example_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "    model = pipeline.fit(example_df)\n",
    "    results = model.transform(example_df).toPandas()\n",
    "    return results['sentence_embeddings'].loc[0][0]['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9176039052884686\n"
     ]
    }
   ],
   "source": [
    "A = get_embedding(\"data\")\n",
    "B = get_embedding(\"the data\")\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = (np.dot(A,B)/(norm(A)*norm(B))+1)/2\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"MATCH (s) WHERE s:Subject or s:Object SET s:SubjectOrObject return *\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "graph = Graph(password='12345678',name=\"version1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in graph.run(\"MATCH (z:SubjectOrObject) RETURN z\"):\n",
    "    #do something with node here\n",
    "    z = r['z']\n",
    "    text = z['text']\n",
    "    if len(text.strip()) > 2:\n",
    "        embedding = get_embedding(text.strip())\n",
    "        z['embedding'] = embedding\n",
    "        z.update()\n",
    "        tx = graph.begin()\n",
    "        tx.push(z)\n",
    "        graph.commit(tx)\n",
    "    else:\n",
    "        embedding = get_embedding(\"THIS IS AN ERROR\")\n",
    "        z['embedding'] = embedding\n",
    "        z.update()\n",
    "        tx = graph.begin()\n",
    "        tx.push(z)\n",
    "        graph.commit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph.run(\"CALL db.index.vector.createNodeIndex('text-embeddings', 'SubjectOrObject', 'embedding', 768, 'cosine')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(No data)\n",
      "(No data)\n"
     ]
    }
   ],
   "source": [
    "print(graph.run(\"match (z:SubjectOrObject)-[e:COSINE_SIM]-(:SubjectOrObject) delete e\"))\n",
    "print(graph.run(\"match (z:SubjectOrObject)-[e:IS_A]-(:Entity) delete e\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_threshold = 0.95\n",
    "cypher = \"MATCH (z:SubjectOrObject) RETURN DISTINCT z.text\";\n",
    "for r in graph.run(cypher):\n",
    "    if len(str(r).strip()) > 2:\n",
    "        sim_cypher = \"\"\"\n",
    "    MATCH (s:SubjectOrObject {text: %s})\n",
    "    CALL db.index.vector.queryNodes('text-embeddings', 20, s.embedding)\n",
    "    YIELD node AS similar, score\n",
    "    WHERE score > %s and elementId(s) < elementId(similar)\n",
    "    MERGE (s)-[:COSINE_SIM {score: score}]-(similar)\n",
    "    return *\n",
    "        \"\"\"%(str(r),sim_threshold)\n",
    "        #print(sim_cypher)\n",
    "        try:\n",
    "            graph.run(sim_cypher)\n",
    "        except:\n",
    "            print(r)\n",
    "            import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>nodeProjection</th><th>relationshipProjection</th><th>graphName</th><th>nodeCount</th><th>relationshipCount</th><th>projectMillis</th></tr><tr><td style=\"text-align:left\">{SubjectOrObject: {label: &#039;SubjectOrObject&#039;, properties: {}}}</td><td style=\"text-align:left\">{COSINE_SIM: {aggregation: &#039;DEFAULT&#039;, orientation: &#039;NATURAL&#039;, indexInverse: false, properties: {}, type: &#039;COSINE_SIM&#039;}}</td><td style=\"text-align:left\">myGraph</td><td style=\"text-align:right\">336</td><td style=\"text-align:right\">551</td><td style=\"text-align:right\">12</td></tr></table>"
      ],
      "text/plain": [
       " nodeProjection                                                | relationshipProjection                                                                                                  | graphName | nodeCount | relationshipCount | projectMillis \n",
       "---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-----------|-----------|-------------------|---------------\n",
       " {SubjectOrObject: {label: 'SubjectOrObject', properties: {}}} | {COSINE_SIM: {aggregation: 'DEFAULT', orientation: 'NATURAL', indexInverse: false, properties: {}, type: 'COSINE_SIM'}} | myGraph   |       336 |               551 |            12 "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph.run(\"CALL gds.graph.drop('myGraph') YIELD graphName;\")\n",
    "except:\n",
    "    pass # graph exists\n",
    "\n",
    "graph.run(\n",
    "    \"\"\"\n",
    "CALL gds.graph.project(\n",
    "  'myGraph',\n",
    "  'SubjectOrObject',\n",
    "  'COSINE_SIM',\n",
    "  {\n",
    "  }\n",
    ")\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL gds.wcc.stream('myGraph')\n",
    "YIELD componentId, nodeId\n",
    "WITH gds.util.asNode(nodeId) AS n, componentId AS componentId\n",
    "SET n.componentId=componentId\n",
    "RETURN n\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "MATCH (z:SubjectOrObject) with COUNT(z.componentId) as c, z.componentId as componentId\n",
    "MERGE (b:Entity {componentId: componentId})\n",
    "return *\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.run(\"MATCH (z:Entity), (b:SubjectOrObject) WHERE b.componentId=z.componentId MERGE (z)<-[:IS_A]-(b) return *\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "MATCH (s:Subject)-[v:VERB]->(o:Object)\n",
    "WITH s,v,o\n",
    "MATCH (e1:Entity {componentId:s.componentId}), (e2:Entity {componentId: o.componentId})\n",
    "with e1,v,e2\n",
    "MERGE (e1)-[:VERB2 {text: v.text, from: elementId(v)}]->(e2) RETURN *\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in graph.run(\"\"\"MATCH (z:Entity) return DISTINCT z.componentId as componentId\"\"\"):\n",
    "    cid = int(str(cid))\n",
    "    texts = []\n",
    "    for text in graph.run(\"\"\"MATCH (z:SubjectOrObject {componentId: %s}) return z.text as text\"\"\"%cid):\n",
    "        texts.append(str(text)[1:-1]) # the weird 1:-1 is because of quoting done by neo4j\n",
    "    text = max(set(texts), key=texts.count)\n",
    "    graph.run(\"\"\"MATCH (z:Entity {componentId: %s}) SET z.text = \"%s\" return *\"\"\"%(cid,text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining and registering vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import SentenceDetector, UniversalSentenceEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "embeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\").setInputCols(\"sentence\").setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler,\n",
    "                            sentence,\n",
    "                            embeddings])\n",
    "\n",
    "def get_embedding2(text):\n",
    "    example_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "    model = pipeline.fit(example_df)\n",
    "    results = model.transform(example_df).toPandas()\n",
    "    return results['sentence_embeddings'].loc[0][0]['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "graph.run(\"match (e:NODE_VERB2)-[r]-() delete e,r\")\n",
    "cypher = \"MATCH (a)-[r:VERB2]->(b) MERGE (e:NODE_VERB2 {text:r.text, from: elementId(a), to: elementId(b), edge: elementId(r)}) RETURN DISTINCT r.text\";\n",
    "graph.run(cypher)\n",
    "for r in graph.run(\"MATCH (z:NODE_VERB2) RETURN z\"):\n",
    "    #do something with node here\n",
    "    z = r['z']\n",
    "    text = z['text']\n",
    "    if len(text.strip()) > 2:\n",
    "        embedding = get_embedding2(text.strip())\n",
    "        z['embedding'] = embedding\n",
    "        z.update()\n",
    "        tx = graph.begin()\n",
    "        tx.push(z)\n",
    "        graph.commit(tx)\n",
    "    else:\n",
    "        embedding = get_embedding2(\"THIS IS AN ERROR\")\n",
    "        z['embedding'] = embedding\n",
    "        z.update()\n",
    "        tx = graph.begin()\n",
    "        tx.push(z)\n",
    "        graph.commit(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph.run(\"CALL db.index.vector.createNodeIndex('edge-text-embeddings', 'NODE_VERB2', 'embedding', 512, 'cosine')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(No data)\n",
      "(No data)\n"
     ]
    }
   ],
   "source": [
    "print(graph.run(\"match (z:NODE_VERB2)-[e:COSINE_SIM]-(:NODE_VERB2) delete e\"))\n",
    "print(graph.run(\"match (z:NODE_VERB2)-[e:IS_A]-(:Entity_VERB2) delete e\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_threshold = 0.9\n",
    "cypher = \"MATCH (z:NODE_VERB2) RETURN DISTINCT z.text\";\n",
    "for r in graph.run(cypher):\n",
    "    if len(str(r).strip()) > 2:\n",
    "        sim_cypher = \"\"\"\n",
    "    MATCH (s:NODE_VERB2 {text: %s})\n",
    "    CALL db.index.vector.queryNodes('edge-text-embeddings', 20, s.embedding)\n",
    "    YIELD node AS similar, score\n",
    "    WHERE score > %s and elementId(s) < elementId(similar)\n",
    "    return elementId(s) as s,elementId(similar) as similar,score\n",
    "        \"\"\"%(str(r),sim_threshold)\n",
    "        for r in graph.run(sim_cypher):\n",
    "            s = r['s']\n",
    "            similar = r['similar']\n",
    "            score = r['score']\n",
    "            graph.run(\"MATCH (s:NODE_VERB2),(similar:NODE_VERB2) where elementId(s)='%s' and elementId(similar)='%s'  MERGE (s)-[e:COSINE_SIM {score: %s}]-(similar) return e.score\"%(s,similar,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>nodeProjection</th><th>relationshipProjection</th><th>graphName</th><th>nodeCount</th><th>relationshipCount</th><th>projectMillis</th></tr><tr><td style=\"text-align:left\">{NODE_VERB2: {label: &#039;NODE_VERB2&#039;, properties: {}}}</td><td style=\"text-align:left\">{COSINE_SIM: {aggregation: &#039;DEFAULT&#039;, orientation: &#039;NATURAL&#039;, indexInverse: false, properties: {}, type: &#039;COSINE_SIM&#039;}}</td><td style=\"text-align:left\">myGraph2</td><td style=\"text-align:right\">168</td><td style=\"text-align:right\">412</td><td style=\"text-align:right\">7</td></tr></table>"
      ],
      "text/plain": [
       " nodeProjection                                      | relationshipProjection                                                                                                  | graphName | nodeCount | relationshipCount | projectMillis \n",
       "-----------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-----------|-----------|-------------------|---------------\n",
       " {NODE_VERB2: {label: 'NODE_VERB2', properties: {}}} | {COSINE_SIM: {aggregation: 'DEFAULT', orientation: 'NATURAL', indexInverse: false, properties: {}, type: 'COSINE_SIM'}} | myGraph2  |       168 |               412 |             7 "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph.run(\"CALL gds.graph.drop('myGraph2') YIELD graphName;\")\n",
    "except:\n",
    "    pass # graph exists\n",
    "\n",
    "graph.run(\n",
    "    \"\"\"\n",
    "CALL gds.graph.project(\n",
    "  'myGraph2',\n",
    "  'NODE_VERB2',\n",
    "  'COSINE_SIM',\n",
    "  {\n",
    "  }\n",
    ")\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL gds.wcc.stream('myGraph2')\n",
    "YIELD componentId, nodeId\n",
    "WITH gds.util.asNode(nodeId) AS n, componentId AS componentId\n",
    "SET n.componentId=componentId\n",
    "RETURN n\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "MATCH (z:NODE_VERB2) with COUNT(z.componentId) as c, z.componentId as componentId\n",
    "MERGE (b:Entity_VERB2 {componentId: componentId})\n",
    "return *\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"MATCH (z:Entity_VERB2), (b:NODE_VERB2) WHERE b.componentId=z.componentId MERGE (z)<-[:IS_A]-(b) return *\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in graph.run(\"\"\"MATCH (z:Entity_VERB2) return DISTINCT z.componentId as componentId\"\"\"):\n",
    "    cid = int(str(cid))\n",
    "    texts = []\n",
    "    for text in graph.run(\"\"\"MATCH (z:NODE_VERB2 {componentId: %s}) return z.text as text\"\"\"%cid):\n",
    "        texts.append(str(text)[1:-1]) # the weird 1:-1 is because of quoting done by neo4j\n",
    "    if len(texts) > 0:\n",
    "        text = max(set(texts), key=texts.count)\n",
    "        graph.run(\"\"\"MATCH (z:Entity_VERB2 {componentId: %s}) SET z.text = \"%s\" return *\"\"\"%(cid,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "MATCH (e1:Entity)-[v:VERB2]-(e2:Entity)\n",
    "MATCH (nv:NODE_VERB2 {text: v.text})-[:IS_A]->(ev:Entity_VERB2)\n",
    "WITH e1,e2,ev\n",
    "MERGE (e1)-[:VERB3 {text: ev.text}]-(e2)\n",
    "RETURN *\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_sentences = pd.DataFrame(changed_sentences,columns=[\"original\",\"from\",\"to\",\"correct (y,n,?)\"])\n",
    "changed_sentences[\"comments\"] = \"\"\n",
    "changed_sentences.to_csv(f\"{SRC_DIR}/coref_resolved/debug_changed.csv\")\n",
    "\n",
    "unchanged_sentences = pd.DataFrame(unchanged_sentences,columns=[\"text\"])\n",
    "unchanged_sentences[\"correct (y,n,?)\"] = \"?\"\n",
    "unchanged_sentences[\"comments\"] = \"\"\n",
    "unchanged_sentences.to_csv(f\"{SRC_DIR}/coref_resolved/debug_unchanged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = graph.run(\"\"\"\n",
    "MATCH (n:SubjectOrObject)\n",
    "WITH n.from AS source, COLLECT(n) AS nodesa\n",
    "UNWIND nodes AS n\n",
    "MATCH (rs:ResultSentence)a\n",
    "WHERE elementID(rs) in source\n",
    "MATCH (n:SubjectOrObject)-[r:VERB]->(m:SubjectOrObject)\n",
    "return rs.text, n.text, r.text, m.text\n",
    "\"\"\")\n",
    "triplets_df = pd.DataFrame(results)\n",
    "triplets_df.columns = [\"source sentence\", \"Subject\", \"Verb\", \"Object\"]\n",
    "\n",
    "results = graph.run(\"\"\"\n",
    "MATCH(n:ResultSentence)-[:FROM]->(m:Paragraph)\n",
    "RETURN n.text, m.text\n",
    "\"\"\")\n",
    "simp_df = pd.DataFrame(results)\n",
    "simp_df.columns = [\"source sentence\", \"source paragraph\"]\n",
    "# simp_df.to_csv(\"par2sent.csv\")\n",
    "\n",
    "triplet_report = pd.merge(simp_df, triplets_df, on=\"source sentence\")\n",
    "triplet_report.to_csv(f\"{REPORT_DIR}/triplet_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.run(\"\"\"\n",
    "MATCH (n:SubjectOrObject)-[r:COSINE_SIM]->(m:SubjectOrObject)\n",
    "RETURN n.text, m.text, r.score\"\"\")\n",
    "score_df = pd.DataFrame(results)\n",
    "score_df.columns = [\"node1\", \"node2\", \"similarity\"]\n",
    "score_df = score_df.drop_duplicates()\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv(\"cosine_similarity_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "name": "10.Clinical_Relation_Extraction",
  "notebookId": 781554605447210,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
