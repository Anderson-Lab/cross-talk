{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bc013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import crosslingual_coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b03ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head': 'Jennifer Anne Doudna', 'type': 'date of birth', 'tail': 'February 19, 1964'}, {'head': 'Jennifer Anne Doudna', 'type': 'award received', 'tail': 'Nobel Prize in Chemistry'}, {'head': 'Emmanuelle Charpentier', 'type': 'award received', 'tail': 'Nobel Prize in Chemistry'}]\n"
     ]
    }
   ],
   "source": [
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "sent = '<s><triplet> Jennifer Anne Doudna <subj> February 19, 1964 <obj> date of birth <subj> Nobel Prize in Chemistry <obj> award received <triplet> Emmanuelle Charpentier <subj> Nobel Prize in Chemistry <obj> award received</s>'\n",
    "print(extract_triplets(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ccbf192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sent = \\'<s><triplet> Jennifer Anne Doudna <subj> February 19, 1964 <obj> date of birth <subj> Nobel Prize in Chemistry <obj> award received <triplet> Emmanuelle Charpentier <subj> Nobel Prize in Chemistry <obj> award received</s>\\'\\ndef extract_triplets(text):\\n    text = text.replace(\"<s>\",\"\")\\n    text = text.replace(\"</s>\",\"\")\\n    if text.startswith(\"<triplet>\"):\\n        text = text[len(\"<triplet>\"):].strip()    \\n    triplets = []\\n    for sent in text.strip().split(\"<triplet>\"):\\n        sent = sent.strip()\\n        print(sent)\\n        parts = []\\n        for token in sent.split(\">\"):\\n            parts.append([w.strip() for w in token.split(\"<\")])\\n        print(len(parts))\\n        assert len(parts)%3 == 0\\n        for n in range(len(parts)//3):\\n            print(n)\\n            start = n*3\\n            end = n*3+3\\n            head,tail,type = [p[0] for p in parts[start:end]]\\n            triplets.append({\\'head\\':head,\"type\":type,\"tail\":tail})\\n    return triplets\\nprint(extract_triplets(sent))'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sent = '<s><triplet> Jennifer Anne Doudna <subj> February 19, 1964 <obj> date of birth <subj> Nobel Prize in Chemistry <obj> award received <triplet> Emmanuelle Charpentier <subj> Nobel Prize in Chemistry <obj> award received</s>'\n",
    "def extract_triplets(text):\n",
    "    text = text.replace(\"<s>\",\"\")\n",
    "    text = text.replace(\"</s>\",\"\")\n",
    "    if text.startswith(\"<triplet>\"):\n",
    "        text = text[len(\"<triplet>\"):].strip()    \n",
    "    triplets = []\n",
    "    for sent in text.strip().split(\"<triplet>\"):\n",
    "        sent = sent.strip()\n",
    "        print(sent)\n",
    "        parts = []\n",
    "        for token in sent.split(\">\"):\n",
    "            parts.append([w.strip() for w in token.split(\"<\")])\n",
    "        print(len(parts))\n",
    "        assert len(parts)%3 == 0\n",
    "        for n in range(len(parts)//3):\n",
    "            print(n)\n",
    "            start = n*3\n",
    "            end = n*3+3\n",
    "            head,tail,type = [p[0] for p in parts[start:end]]\n",
    "            triplets.append({'head':head,\"type\":type,\"tail\":tail})\n",
    "    return triplets\n",
    "print(extract_triplets(sent))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a23f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def call_wiki_api(item):\n",
    "  try:\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "    data = requests.get(url).json()\n",
    "    # Return the first id (Could upgrade this in the future)\n",
    "    return data['search'][0]['id']\n",
    "  except:\n",
    "    return 'id-less'\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": -1,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "          Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "          return mapping\n",
    "        else:\n",
    "          res = call_wiki_api(item)\n",
    "          self.entity_mapping[item] = res\n",
    "          return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9fd036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/site-packages/allennlp/models/archival.py:328: UserWarning: The model models/crosslingual-coreference/minilm/model.tar.gz was trained on a newer version of AllenNLP (v2.9.3), but you're using version 2.10.1.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RebelComponent at 0x7fa682ce6a10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n",
    "\n",
    "# Define rel extraction model\n",
    "\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c174a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385d9ac30c35635c7e36a1636ded4a091f830cf3: {'relation': 'country of citizenship', 'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'}, 'tail_span': {'text': 'Germany', 'id': 'Q183'}}\n",
      "471a35571b66cc8e1e3415e27f7086505310efc6: {'relation': 'employer', 'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'}, 'tail_span': {'text': 'Google', 'id': 'Q95'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"Christian Drosten works in Germany. He likes to work for Google.\"\n",
    "\n",
    "coref_text = coref(input_text)._.resolved_text\n",
    "\n",
    "doc = rel_ext(coref_text)\n",
    "\n",
    "for value, rel_dict in doc._.rel.items():\n",
    "    print(f\"{value}: {rel_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b7ffd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Christian Drosten works in Germany. Christian Drosten likes to work for Google.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14b62b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'385d9ac30c35635c7e36a1636ded4a091f830cf3': {'relation': 'country of citizenship',\n",
       "  'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'},\n",
       "  'tail_span': {'text': 'Germany', 'id': 'Q183'}},\n",
       " '471a35571b66cc8e1e3415e27f7086505310efc6': {'relation': 'employer',\n",
       "  'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'},\n",
       "  'tail_span': {'text': 'Google', 'id': 'Q95'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b09f32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Define Neo4j connection\n",
    "host = 'bolt://127.0.0.1:7687'\n",
    "user = 'neo4j'\n",
    "password = '12345678'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb0b2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j.api.ServerInfo at 0x7fa63f7f8a10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get_server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f88283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (h:Entity {id: CASE WHEN NOT row.head_span.id = 'id-less' THEN row.head_span.id ELSE row.head_span.text END})\n",
    "ON CREATE SET h.text = row.head_span.text\n",
    "MERGE (t:Entity {id: CASE WHEN NOT row.tail_span.id = 'id-less' THEN row.tail_span.id ELSE row.tail_span.text END})\n",
    "ON CREATE SET t.text = row.tail_span.text\n",
    "WITH row, h, t\n",
    "CALL apoc.merge.relationship(h, toUpper(replace(row.relation,' ', '_')),\n",
    "  {},\n",
    "  {},\n",
    "  t,\n",
    "  {}\n",
    ")\n",
    "YIELD rel\n",
    "RETURN distinct 'done' AS result;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_query(query, params={}):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())\n",
    "\n",
    "def store_wikipedia_summary(page):\n",
    "  #try:\n",
    "  input_text = wikipedia.page(page).summary\n",
    "  print(input_text)\n",
    "  coref_text = coref(input_text)._.resolved_text\n",
    "  doc = rel_ext(coref_text)\n",
    "  params = [rel_dict for value, rel_dict in doc._.rel.items()]\n",
    "  run_query(import_query, {'data': params})\n",
    "  #except Exception as e:\n",
    "  #  print(f\"Couldn't parse text for {page} due to {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f01c91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Jennifer Doudna\n",
      "Jennifer Anne Doudna  (; born February 19, 1964) is an American biochemist who has done pioneering work in CRISPR gene editing, and made other fundamental contributions in biochemistry and genetics. Doudna was one of the first women to share a Nobel in the sciences. She received the 2020 Nobel Prize in Chemistry, with Emmanuelle Charpentier, \"for the development of a method for genome editing.\" She is the Li Ka Shing Chancellor's Chair Professor in the department of chemistry and the department of molecular and cell biology at the University of California, Berkeley. She has been an investigator with the Howard Hughes Medical Institute since 1997.\n",
      "Doudna graduated from Pomona College in 1985 and earned a Ph.D. from Harvard Medical School in 1989. Apart from her professorship at Berkeley, she is also president and chair of the board of the Innovative Genomics Institute, a faculty scientist at Lawrence Berkeley National Laboratory, a senior investigator at the Gladstone Institutes, and an adjunct professor of cellular and molecular pharmacology at the University of California, San Francisco (UCSF). In 2012, Doudna and Emmanuelle Charpentier were the first to propose that CRISPR-Cas9 (enzymes from bacteria that control microbial immunity) could be used for programmable editing of genomes, which has been called one of the most significant discoveries in the history of biology. Since then, Doudna has been a leading figure in what is referred to as the \"CRISPR revolution\" for her fundamental work and leadership in developing CRISPR-mediated genome editing. Her many other awards and fellowships include the 2000 Alan T. Waterman Award for her research on the structure of a ribozyme, as determined by X-ray crystallography and the 2015 Breakthrough Prize in Life Sciences for CRISPR-Cas9 genome editing technology, with Charpentier. She has been a co-recipient of the Gruber Prize in Genetics (2015), the Tang Prize (2016), the Canada Gairdner International Award (2016), and the Japan Prize (2017). She was  named one of the Time 100 most influential people in 2015.\n",
      "Parsing Rachel Carson\n",
      "Rachel Louise Carson (May 27, 1907 – April 14, 1964) was an American marine biologist, writer, and conservationist whose influential book Silent Spring (1962) and other writings are credited with advancing the global environmental movement.\n",
      "Carson began her career as an aquatic biologist in the U.S. Bureau of Fisheries, and became a full-time nature writer in the 1950s. Her widely praised 1951 bestseller The Sea Around Us won her a U.S. National Book Award, recognition as a gifted writer and financial security. Her next book, The Edge of the Sea, and the reissued version of her first book, Under the Sea Wind, were also bestsellers. This sea trilogy explores the whole of ocean life from the shores to the depths.\n",
      "Late in the 1950s, Carson turned her attention to conservation, especially some problems she believed were caused by synthetic pesticides. The result was the book Silent Spring (1962), which brought environmental concerns to an unprecedented share of the American people. Although Silent Spring was met with fierce opposition by chemical companies, it spurred a reversal in national pesticide policy, which led to a nationwide ban on DDT and other pesticides. It also inspired a grassroots environmental movement that led to the creation of the U.S. Environmental Protection Agency. Carson was posthumously awarded the Presidential Medal of Freedom by President Jimmy Carter.\n",
      "\n",
      "\n",
      "Parsing Sara Seager OC\n",
      "Sara Seager  (born 21 July 1971) is a Canadian-American astronomer and planetary scientist. She is a professor at the Massachusetts Institute of Technology and is known for her work on extrasolar planets and their atmospheres. She is the author of two textbooks on these topics, and has been recognized for her research by Popular Science, Discover Magazine, Nature, and TIME Magazine. Seager was awarded a MacArthur Fellowship in 2013 citing her theoretical work on detecting chemical signatures on exoplanet atmospheres and developing low-cost space observatories to observe planetary transits.\n",
      "Parsing Gertrude Elion\n",
      "Gertrude \"Trudy\" Belle Elion (January 23, 1918 – February 21, 1999) was an American biochemist and pharmacologist, who shared the 1988 Nobel Prize in Physiology or Medicine with George H. Hitchings and Sir James Black for their use of innovative methods of rational drug design for the development of new drugs. This new method focused on understanding the target of the drug rather than simply using trial-and-error. Her work led to the creation of the anti-retroviral drug AZT, which was the first drug widely used against AIDS. Her well known works also include the development of the first immunosuppressive drug, azathioprine, used to fight rejection in organ transplants, and the first successful antiviral drug, acyclovir (ACV), used in the treatment of herpes infection.\n",
      "Parsing Rita Levi-Montalcini\n",
      "Rita Levi-Montalcini  (US: , Italian: [ˈriːta ˈlɛːvi montalˈtʃiːni]; 22 April 1909 – 30 December 2012) was an Italian neurobiologist. She was awarded the 1986 Nobel Prize in Physiology or Medicine jointly with colleague Stanley Cohen for the discovery of nerve growth factor (NGF).From 2001 until her death, she also served in the Italian Senate as a Senator for Life. This honor was given due to her significant scientific contributions. On 22 April 2009, she became the first Nobel laureate to reach the age of 100, and the event was feted with a party at Rome's City Hall.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ladies = [\"Jennifer Doudna\", \"Rachel Carson\", \"Sara Seager OC\", \"Gertrude Elion\", \"Rita Levi-Montalcini\"]\n",
    "\n",
    "for l in ladies:\n",
    "  print(f\"Parsing {l}\")\n",
    "  store_wikipedia_summary(l)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab1a517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batches</th>\n",
       "      <th>total</th>\n",
       "      <th>timeTaken</th>\n",
       "      <th>committedOperations</th>\n",
       "      <th>failedOperations</th>\n",
       "      <th>failedBatches</th>\n",
       "      <th>retries</th>\n",
       "      <th>errorMessages</th>\n",
       "      <th>batch</th>\n",
       "      <th>operations</th>\n",
       "      <th>wasTerminated</th>\n",
       "      <th>failedParams</th>\n",
       "      <th>updateStatistics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Cannot merge the following node because of n...</td>\n",
       "      <td>{'total': 50, 'committed': 46, 'failed': 4, 'e...</td>\n",
       "      <td>{'total': 50, 'committed': 46, 'failed': 4, 'e...</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'nodesDeleted': 0, 'labelsAdded': 0, 'relatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batches  total  timeTaken  committedOperations  failedOperations  \\\n",
       "0       50     50          0                   46                 4   \n",
       "\n",
       "   failedBatches  retries                                      errorMessages  \\\n",
       "0              4        0  {'Cannot merge the following node because of n...   \n",
       "\n",
       "                                               batch  \\\n",
       "0  {'total': 50, 'committed': 46, 'failed': 4, 'e...   \n",
       "\n",
       "                                          operations  wasTerminated  \\\n",
       "0  {'total': 50, 'committed': 46, 'failed': 4, 'e...          False   \n",
       "\n",
       "  failedParams                                   updateStatistics  \n",
       "0           {}  {'nodesDeleted': 0, 'labelsAdded': 0, 'relatio...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_query(\"\"\"\n",
    "CALL apoc.periodic.iterate(\"\n",
    "  MATCH (e:Entity)\n",
    "  WHERE e.id STARTS WITH 'Q'\n",
    "  RETURN e\n",
    "\",\"\n",
    "  // Prepare a SparQL query\n",
    "  WITH 'SELECT * WHERE{ ?item rdfs:label ?name . filter (?item = wd:' + e.id + ') filter (lang(?name) = \\\\\"en\\\\\") ' +\n",
    "     'OPTIONAL {?item wdt:P31 [rdfs:label ?label] .filter(lang(?label)=\\\\\"en\\\\\")}}' AS sparql, e\n",
    "  // make a request to Wikidata\n",
    "  CALL apoc.load.jsonParams(\n",
    "    'https://query.wikidata.org/sparql?query=' + \n",
    "      + apoc.text.urlencode(sparql),\n",
    "      { Accept: 'application/sparql-results+json'}, null)\n",
    "  YIELD value\n",
    "  UNWIND value['results']['bindings'] as row\n",
    "  SET e.wikipedia_name = row.name.value\n",
    "  WITH e, row.label.value AS label\n",
    "  MERGE (c:Class {id:label})\n",
    "  MERGE (e)-[:INSTANCE_OF]->(c)\n",
    "  RETURN distinct 'done'\", {batchSize:1, retry:1})\n",
    "\"\"\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e57c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
